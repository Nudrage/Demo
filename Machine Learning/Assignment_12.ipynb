{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is prior probability? Give an example.\n",
    "\n",
    "\n",
    "Prior probability, in the context of Bayesian statistics, refers to the initial belief or probability assigned to an event or hypothesis before any evidence is taken into account. It represents what we know or believe about the probability of an event based on prior knowledge, experience, or assumptions. Prior probabilities are often updated using Bayes' theorem as new evidence becomes available. This process allows us to make more informed decisions or conclusions by combining prior beliefs with observed data.\n",
    "\n",
    "For example, let's consider a medical diagnosis scenario where a patient is being tested for a rare disease. The prior probability of the patient having the disease would be the probability assigned to this event before any test results are considered. If, based on previous research or medical history, we know that only 1 in 1,000 people in the general population have this disease, then the prior probability of a randomly chosen individual having the disease would be 0.001 or 0.1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is posterior probability? Give an example.\n",
    "\n",
    "\n",
    "Posterior probability, in Bayesian statistics, refers to the updated probability of a hypothesis or event after taking into account new evidence or data. It represents the revised belief or probability based on both prior knowledge and observed data.\n",
    "\n",
    "For example, let's revisit the medical diagnosis scenario mentioned earlier, where a patient is being tested for a rare disease. Suppose the prior probability of the patient having the disease based on previous research or medical history is 0.001 or 0.1%. Now, if the patient undergoes a diagnostic test that comes back positive, we can calculate the posterior probability of the patient having the disease using Bayes' theorem.\n",
    "\n",
    "Let's say the sensitivity of the diagnostic test (the probability of a positive test result given that the patient has the disease) is 0.95, and the specificity of the test (the probability of a negative test result given that the patient does not have the disease) is 0.90. If we assume that the patient's test result is accurate, we can calculate the posterior probability using Bayes' theorem.\n",
    "\n",
    "Suppose we denote:\n",
    "- \\( P(D) \\): Prior probability of the patient having the disease (0.001)\n",
    "- \\( P(\\neg D) \\): Prior probability of the patient not having the disease (1 - 0.001 = 0.999)\n",
    "- \\( P(+|D) \\): Sensitivity of the test (0.95)\n",
    "- \\( P(-|\\neg D) \\): Specificity of the test (0.90)\n",
    "\n",
    "Using Bayes' theorem, the posterior probability \\( P(D|+) \\) of the patient having the disease given a positive test result is:\n",
    "\n",
    "\\[ P(D|+) = $\\frac{{P(+|D) \\times P(D)}}{{P(+|D) \\times P(D) + P(+|\\neg D) \\times P(\\neg D)}}$ \\]\n",
    "\n",
    "\\[ P(D|+) = $\\frac{{0.95 \\times 0.001}}{{0.95 \\times 0.001 + (1-0.90) \\times 0.999}}$ \\]\n",
    "\n",
    "\\[ P(D|+) = $\\frac{{0.00095}}{{0.00095 + 0.0999}}$ \\]\n",
    "\n",
    "\\[ P(D|+) ≈ 0.00946 \\]\n",
    "\n",
    "So, the posterior probability of the patient having the disease after receiving a positive test result is approximately 0.946%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is likelihood probability? Give an example.\n",
    "\n",
    "\n",
    "Likelihood probability, in statistics, specifically in Bayesian inference, refers to the probability of observing the data given a particular hypothesis or set of parameters. It represents how well the observed data support or fit a specific hypothesis or model.In Bayesian statistics, likelihood is distinct from probability. Probability quantifies the uncertainty of events before they occur, while likelihood quantifies the support for different hypotheses or parameter values after observing the data.\n",
    "\n",
    "Mathematically, the likelihood function \\( L($\\theta$ | x) \\) is defined as the probability density function (or probability mass function) of the observed data \\( x \\), given a parameter or hypothesis \\( $\\theta$ \\). It's often denoted as \\( f(x|$\\theta$) \\) or \\( p(x|$\\theta$) \\).\n",
    "\n",
    "For example, consider a coin-flipping experiment. Let's say we're interested in determining whether the coin is fair (has an equal probability of landing heads or tails) or biased. We flip the coin 10 times and observe that it lands heads 7 times and tails 3 times.\n",
    "\n",
    "- The likelihood function in this case would represent the probability of obtaining 7 heads and 3 tails in 10 coin flips, given different hypotheses about the fairness of the coin.\n",
    "\n",
    "- If we denote \\( $\\theta$ \\) as the probability of the coin landing heads (which would be 0.5 for a fair coin), the likelihood function \\( L($\\theta$ | x) \\) would quantify how likely it is to observe the data (7 heads and 3 tails) for different values of \\( $\\theta$ \\).\n",
    "\n",
    "- For example, if we assume the coin is fair (\\( $\\theta$ = 0.5 \\)), the likelihood of observing 7 heads and 3 tails in 10 flips would be calculated using the binomial distribution. If we assume the coin is biased (\\( $\\theta \\neq 0.5$ \\)), the likelihood would be calculated accordingly.\n",
    "\n",
    "The likelihood function helps us assess which hypotheses or parameter values are most supported by the observed data, facilitating inference and decision-making in Bayesian analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is Naïve Bayes classifier? Why is it named so?\n",
    "\n",
    "\n",
    "The Naïve Bayes classifier is a simple probabilistic classifier based on Bayes' theorem with strong (naïve) independence assumptions between the features. Despite its simplicity, Naïve Bayes classifiers are often effective in various real-world applications, such as text classification, spam filtering, and sentiment analysis.\n",
    "\n",
    "The classifier is called \"naïve\" because it makes the strong assumption that the features used to predict the class labels are conditionally independent given the class. In other words, it assumes that the presence or absence of a particular feature is unrelated to the presence or absence of any other feature, given the class label. This is a simplifying assumption that allows the classifier to be computationally efficient and easy to implement, but it may not hold true in practice for all datasets.\n",
    "\n",
    "The name \"Naïve Bayes\" comes from the use of Bayes' theorem in the classifier along with the assumption of feature independence. Bayes' theorem provides a way to update the probability of a hypothesis (class label) given evidence (features), and the naïve assumption simplifies the calculations by assuming independence between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What is optimal Bayes classifier?\n",
    "\n",
    "\n",
    "The Optimal Bayes Classifier is a theoretical classifier that minimizes the misclassification rate and assigns each observation to the class with the highest posterior probability. It's considered \"optimal\" because, under certain conditions, it achieves the lowest possible error rate among all classifiers. In mathematical terms, the Optimal Bayes Classifier assigns an observation \\(x\\) to the class \\($C_{i}$\\) that maximizes the posterior probability \\(P($C_{i}$ | x)\\): \\[ $\\text{Classify } x \\text{ as } C_{i} \\text{ if } i = \\arg \\max_{i} P(C_{i} | x)$ \\]\n",
    "\n",
    "Here, \\( $\\arg \\max$ \\) denotes the class that maximizes the posterior probability. The posterior probability \\(P($C_{i}$ | x)\\) is computed using Bayes' theorem: \\[ P($C_{i}$ | x) = $\\frac{P(x | C_{i}) \\times P(C_{i})}{P(x)}$ \\]\n",
    "\n",
    "Where:\n",
    "- \\( P($C_{i}$ | x) \\) is the posterior probability of class \\($C_{i}$\\) given observation \\(x\\).\n",
    "- \\( P(x | $C_{i}$) \\) is the likelihood of observation \\(x\\) given class \\($C_{i}$\\).\n",
    "- \\( P($C_{i}$) \\) is the prior probability of class \\($C_{i}$\\).\n",
    "- \\( P(x) \\) is the probability of observation \\(x\\).\n",
    "\n",
    "The Optimal Bayes Classifier directly computes the posterior probabilities for each class and selects the class with the highest probability for classification. It's important to note that while the Optimal Bayes Classifier is theoretically optimal, it requires knowledge of the true class conditional densities \\(P(x | $C_{i}$)\\) and the prior probabilities \\(P($C_{i}$)\\), which are often unknown in practice and need to be estimated from the data. Additionally, it assumes that the cost of misclassification is equal for all classes, which may not always be the case in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Write any two features of Bayesian learning methods.\n",
    "\n",
    "\n",
    "Two features of Bayesian learning methods are:\n",
    "1. **Incorporation of Prior Knowledge**: Bayesian learning methods allow the incorporation of prior knowledge or beliefs about the parameters or structure of a model into the learning process.  By incorporating prior knowledge, Bayesian methods can make more informed and reliable predictions, especially when dealing with small or limited datasets.\n",
    "2. **Quantification of Uncertainty**: Bayesian learning methods provide a principled framework for quantifying uncertainty in predictions. Instead of providing point estimates of parameters or predictions, Bayesian methods yield probability distributions over these quantities. This allows decision-makers to assess the uncertainty associated with predictions and make decisions that account for this uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Define the concept of consistent learners.\n",
    "\n",
    "\n",
    "In machine learning, consistent learners are algorithms or models that converge to the true underlying function or distribution generating the data as the amount of training data increases indefinitely. In other words, a consistent learner will eventually learn the correct relationship between input features and target outputs given an infinite amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Write any two strengths of Bayes classifier.\n",
    "\n",
    "\n",
    "Two strengths of the Bayes classifier are:\n",
    "1. **Probabilistic Framework**: The Bayes classifier provides a probabilistic framework for classification, allowing it to quantify uncertainty and provide confidence estimates for its predictions. Instead of just assigning a class label to each observation, the Bayes classifier computes the posterior probability of each class given the observed features. This probabilistic approach enables more nuanced decision-making, especially in situations where misclassification costs are not uniform across classes or where uncertainty in predictions is critical.\n",
    "2. **Robustness to Irrelevant Features**: The Bayes classifier is known to be robust to irrelevant features in the dataset. Since the classifier computes the posterior probability of each class based on the observed features, irrelevant features that do not contribute useful information to the classification task are effectively ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Write any two weaknesses of Bayes classifier.\n",
    "\n",
    "\n",
    "Two weaknesses of the Bayes classifier are:\n",
    "1. **Strong Independence Assumption**: The Bayes classifier relies on the strong assumption of feature independence given the class label (i.e., features are conditionally independent). While this assumption simplifies computations and makes the classifier computationally efficient, it may not hold true in practice for many real-world datasets.\n",
    "2. **Sensitivity to Prior Probabilities**: The Bayes classifier's predictions are sensitive to the choice of prior probabilities assigned to each class. In situations where reliable prior information is unavailable or difficult to specify, the choice of prior probabilities can significantly impact the classifier's performance. Biased or incorrect prior probabilities can lead to biased predictions and poor generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Explain how Naïve Bayes classifier is used for\n",
    "\n",
    "**1. Text classification :** In text classification, the goal is to automatically assign predefined categories or labels to text documents based on their content. This task is commonly encountered in applications such as document categorization, topic labeling, sentiment analysis, and spam detection. To use the Naïve Bayes classifier for text classification, each document is represented as a bag-of-words or bag-of-ngrams, where the frequency of occurrence of each word or ngram in the document is used as a feature vector. The Naïve Bayes classifier then learns the conditional probability distribution of each class label given the feature vector (i.e., the words or ngrams in the document). During classification, the classifier computes the posterior probability of each class label given the feature vector using Bayes' theorem and selects the class with the highest posterior probability as the predicted label for the document.\n",
    "\n",
    "**2. Spam filtering :** In spam filtering, the task is to automatically classify incoming email messages as either spam or legitimate (ham) based on their content. To use the Naïve Bayes classifier for spam filtering, each email message is represented as a bag-of-words or bag-of-ngrams, similar to text classification. The Naïve Bayes classifier is trained on a dataset of labeled email messages (spam or ham), where the frequency of occurrence of each word or ngram in the email is used as a feature vector. During classification, the classifier computes the posterior probability of each class label (spam or ham) given the feature vector and assigns the email to the class with the highest posterior probability. Email messages with a high probability of being spam are filtered out or flagged for further review.\n",
    "\n",
    "**3. Market sentiment analysis :** In market sentiment analysis, the task is to analyze and predict the sentiment or mood of market participants (e.g., investors, traders) based on textual data such as news articles, social media posts, and financial reports. To use the Naïve Bayes classifier for market sentiment analysis, textual data related to market events or discussions is collected and labeled with sentiment labels (e.g., positive, negative, neutral). Each piece of textual data is represented as a bag-of-words or bag-of-ngrams, where the frequency of occurrence of each word or ngram is used as a feature vector. The Naïve Bayes classifier is trained on the labeled dataset to learn the conditional probability distribution of sentiment labels given the feature vectors. During analysis, the classifier computes the posterior probability of each sentiment label given the feature vector and predicts the sentiment of market participants based on the label with the highest posterior probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
