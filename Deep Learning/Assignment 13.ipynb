{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical\n",
    "Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training\n",
    "algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression\n",
    "classifier?\n",
    "\n",
    "Logistic Regression and the classical Perceptron are both linear classifiers, but there are important differences between them. Generally, Logistic Regression is preferred over the classical Perceptron for several reasons:\n",
    "\n",
    "1. **Output Range:**\n",
    "   - **Logistic Regression:** The output of a logistic regression model is a probability estimate, bounded between 0 and 1. This makes it suitable for binary classification tasks, where the output can be interpreted as the probability of belonging to a certain class.\n",
    "   - **Perceptron:** The output of a classical perceptron is binary (0 or 1), making it less flexible for probability estimation.\n",
    "\n",
    "2. **Differentiability:**\n",
    "   - **Logistic Regression:** The logistic function used in logistic regression is smooth and differentiable. This differentiability is crucial for gradient-based optimization algorithms, allowing efficient and stable training.\n",
    "   - **Perceptron:** The step function used in the classical perceptron is not differentiable, which can cause optimization issues when using gradient-based methods.\n",
    "\n",
    "3. **Gradient Descent:**\n",
    "   - **Logistic Regression:** Logistic Regression can be trained using gradient descent or other optimization methods that require the computation of gradients. The smoothness of the logistic function facilitates stable and efficient optimization.\n",
    "   - **Perceptron:** The lack of differentiability in the step function makes it challenging to use traditional gradient descent. The Perceptron training algorithm involves updating weights based on misclassified examples, and it doesn't have the same smooth optimization properties as logistic regression.\n",
    "\n",
    "To make a Perceptron equivalent to a Logistic Regression classifier, you can make the following modifications:\n",
    "\n",
    "1. **Activation Function:**\n",
    "   - Replace the step function in the Perceptron with a logistic (sigmoid) activation function. This transforms the output of the Perceptron into a continuous range between 0 and 1.\n",
    "\n",
    "2. **Loss Function:**\n",
    "   - Use a logistic loss (cross-entropy) instead of the Perceptron's original loss function. The logistic loss is suitable for probabilistic interpretation and is compatible with gradient-based optimization methods.\n",
    "\n",
    "By incorporating these changes, you essentially transform the classical Perceptron into a logistic regression model. The resulting model will have a similar structure to logistic regression and exhibit the desirable properties associated with logistic regression, such as smoothness, differentiability, and suitability for probability estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "The logistic activation function, also known as the sigmoid activation function, played a key role in training the first Multilayer Perceptrons (MLPs) for several reasons:\n",
    "\n",
    "1. **Differentiability:**\n",
    "   - The logistic function is differentiable for all values of its input. This differentiability is crucial for training neural networks using gradient-based optimization algorithms like gradient descent. The ability to compute derivatives allows efficient backpropagation of errors through the network, enabling the adjustment of weights to minimize the error.\n",
    "\n",
    "2. **Squashing Nonlinearity:**\n",
    "   - The logistic function \"squashes\" its input into a range between 0 and 1. This characteristic is important for introducing nonlinearity into the network. Without nonlinearity, stacking multiple layers of neurons would not provide any additional representational power. The logistic function introduces the necessary nonlinearity, allowing the network to learn complex relationships in the data.\n",
    "\n",
    "3. **Output as Probabilities:**\n",
    "   - The logistic function produces outputs in the range [0, 1], which can be interpreted as probabilities. In binary classification problems, the output of the logistic activation function can represent the probability of belonging to the positive class. This probability interpretation is crucial for tasks where understanding the uncertainty or confidence of predictions is important.\n",
    "\n",
    "4. **Smooth Transitions:**\n",
    "   - The logistic function ensures smooth transitions between different input values. This smoothness is beneficial for optimization algorithms, making it easier for them to find a global minimum during training. In contrast, using step functions (as in the original perceptron) would result in non-differentiable points, making optimization more challenging.\n",
    "\n",
    "5. **Vanishing Gradient Mitigation:**\n",
    "   - The logistic function helps mitigate the vanishing gradient problem. In the backpropagation algorithm, gradients are propagated backward through the layers during training. The logistic function, with its non-zero derivatives even for extreme values, helps maintain non-zero gradients and avoids the issue of gradients becoming too small (vanishing) as they are backpropagated through multiple layers.\n",
    "\n",
    "While the logistic activation function was a key ingredient in training the first MLPs, subsequent research has introduced alternative activation functions, such as the hyperbolic tangent (tanh) and rectified linear unit (ReLU), which have become popular choices in modern neural networks. These newer activation functions address some of the limitations of the logistic function, such as the vanishing gradient problem and the restricted output range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Name three popular activation functions. Can you draw them?\n",
    "Three popular activation functions used in neural networks are:\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit):**\n",
    "   - **Function:** \\(f(x) = \\max(0, x)\\)\n",
    "   - **Advantages:** ReLU is computationally efficient and helps mitigate the vanishing gradient problem. It introduces non-linearity into the network and has been widely adopted in many deep learning architectures.\n",
    "\n",
    "2. **Sigmoid (Logistic):**\n",
    "   - **Function:** \\(f(x) = \\frac{1}{1 + e^{-x}}\\)\n",
    "   - **Advantages:** Sigmoid squashes its input to the range \\((0, 1)\\), making it suitable for binary classification problems where the output can be interpreted as a probability. However, it has a tendency to saturate for extreme input values, leading to vanishing gradients.\n",
    "\n",
    "3. **tanh (Hyperbolic Tangent):**\n",
    "   - **Function:** \\(f(x) = \\frac{e^{2x} - 1}{e^{2x} + 1}\\)\n",
    "   - **Advantages:** The tanh function squashes its input to the range \\((-1, 1)\\), making it zero-centered. This can help mitigate issues related to gradient vanishing, which are present in the sigmoid function. tanh is commonly used in scenarios where zero-centered outputs are desired.\n",
    "\n",
    "These activation functions introduce non-linearity into the neural network, allowing it to learn complex relationships in the data. The choice of activation function depends on the specific characteristics of the problem, the architecture of the network, and empirical performance on the task at hand. Additionally, newer activation functions, such as variants of ReLU (e.g., Leaky ReLU, Parametric ReLU) and advanced activations like Swish, have been proposed and used in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Suppose you have an MLP composed of one input layer with 10 passthrough neurons,\n",
    "followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3\n",
    "artificial neurons. All artificial neurons use the ReLU activation function.\n",
    " What is the shape of the input matrix X?\n",
    " What about the shape of the hidden layer’s weight vector Wh, and the shape of its\n",
    "bias vector bh?\n",
    " What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    " What is the shape of the network’s output matrix Y?\n",
    " Write the equation that computes the network’s output matrix Y as a function\n",
    "of X, Wh, bh, Wo and bo.\n",
    "\n",
    "Let's denote the following:\n",
    "\n",
    "- \\(X\\) as the input matrix of shape \\((\\text{batch size}, \\text{input features})\\),\n",
    "- \\(Wh\\) as the weight matrix for the hidden layer of shape \\((\\text{input features}, \\text{number of hidden neurons})\\),\n",
    "- \\(bh\\) as the bias vector for the hidden layer of shape \\((\\text{number of hidden neurons},)\\),\n",
    "- \\(Wo\\) as the weight matrix for the output layer of shape \\((\\text{number of hidden neurons}, \\text{number of output neurons})\\),\n",
    "- \\(bo\\) as the bias vector for the output layer of shape \\((\\text{number of output neurons},)\\),\n",
    "- \\(Y\\) as the output matrix of shape \\((\\text{batch size}, \\text{number of output neurons})\\).\n",
    "\n",
    "Now, based on the information provided:\n",
    "\n",
    "1. **Input Matrix \\(X\\):**\n",
    "   - Shape: \\((\\text{batch size}, 10)\\)\n",
    "\n",
    "2. **Hidden Layer's Weight Matrix \\(Wh\\):**\n",
    "   - Shape: \\((10, 50)\\)\n",
    "\n",
    "3. **Hidden Layer's Bias Vector \\(bh\\):**\n",
    "   - Shape: \\((50,)\\)\n",
    "\n",
    "4. **Output Layer's Weight Matrix \\(Wo\\):**\n",
    "   - Shape: \\((50, 3)\\)\n",
    "\n",
    "5. **Output Layer's Bias Vector \\(bo\\):**\n",
    "   - Shape: \\((3,)\\)\n",
    "\n",
    "6. **Network's Output Matrix \\(Y\\):**\n",
    "   - Shape: \\((\\text{batch size}, 3)\\)\n",
    "\n",
    "7. **Equation for the Network's Output \\(Y\\):**\n",
    "   - The output of the hidden layer can be computed using the ReLU activation function, and the final output of the network can be obtained by applying the ReLU activation to the weighted sum at the output layer.\n",
    "\n",
    "   The equations are as follows:\n",
    "\n",
    "   \\[\\text{Hidden Layer Output (without activation): } H = X \\cdot Wh + bh\\]\n",
    "   \\[\\text{Hidden Layer Output (with ReLU activation): } A = \\max(0, H)\\]\n",
    "   \\[\\text{Network Output: } Y = A \\cdot Wo + bo\\]\n",
    "\n",
    "In matrix form:\n",
    "\n",
    "\\[Y = \\max(0, X \\cdot Wh + bh) \\cdot Wo + bo\\]\n",
    "\n",
    "These equations represent the forward pass of the given MLP, where \\(X\\) is the input matrix, \\(Wh\\) and \\(Wo\\) are the weight matrices for the hidden and output layers, \\(bh\\) and \\(bo\\) are the bias vectors, and \\(\\max(0, \\cdot)\\) denotes the element-wise ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How many neurons do you need in the output layer if you want to classify email into spam\n",
    "or ham? What activation function should you use in the output layer? If instead you want to\n",
    "tackle MNIST, how many neurons do you need in the output layer, using what activation\n",
    "function?\n",
    "\n",
    "### Email Classification (Spam or Ham):\n",
    "\n",
    "1. **Number of Neurons in Output Layer:**\n",
    "   - For binary classification (spam or ham), you need one neuron in the output layer.\n",
    "  \n",
    "2. **Activation Function in Output Layer:**\n",
    "   - Use the sigmoid activation function in the output layer for binary classification. The sigmoid function squashes the output between 0 and 1, representing the probability of the input belonging to the positive class (spam).\n",
    "\n",
    "### MNIST Classification:\n",
    "\n",
    "1. **Number of Neurons in Output Layer:**\n",
    "   - For MNIST, which has 10 classes (digits 0 through 9), you need 10 neurons in the output layer, each representing a different digit.\n",
    "\n",
    "2. **Activation Function in Output Layer:**\n",
    "   - Use the softmax activation function in the output layer for multi-class classification. The softmax function converts the raw output scores into probability distributions over the different classes, facilitating the interpretation of the network's confidence in each class.\n",
    "\n",
    "In summary:\n",
    "- **Email Classification:**\n",
    "  - Number of Neurons in Output Layer: 1\n",
    "  - Activation Function in Output Layer: Sigmoid\n",
    "\n",
    "- **MNIST Classification:**\n",
    "  - Number of Neurons in Output Layer: 10\n",
    "  - Activation Function in Output Layer: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is backpropagation and how does it work? What is the difference between\n",
    "backpropagation and reverse-mode autodiff?\n",
    "\n",
    "### Backpropagation:\n",
    "\n",
    "**Backpropagation** is a supervised learning algorithm used to train neural networks. It consists of two phases: the forward pass and the backward pass.\n",
    "\n",
    "1. **Forward Pass:**\n",
    "   - During the forward pass, the input data is passed through the network, layer by layer, to compute the predicted output.\n",
    "   - The input is multiplied by the weights, and activation functions are applied to produce the output of each layer.\n",
    "   - The predicted output is then compared to the actual target values using a loss function, which quantifies the difference between the predicted and actual values.\n",
    "\n",
    "2. **Backward Pass:**\n",
    "   - The backward pass involves calculating the gradient of the loss function with respect to the weights of the network.\n",
    "   - The gradients are computed using the chain rule of calculus, starting from the output layer and moving backward through the network.\n",
    "   - The gradients indicate how much the loss would increase or decrease if the weights were adjusted.\n",
    "\n",
    "3. **Weight Update:**\n",
    "   - The computed gradients are used to update the weights of the network in the direction that minimizes the loss.\n",
    "   - This process is typically performed using an optimization algorithm like gradient descent.\n",
    "\n",
    "### Reverse-Mode Autodiff:\n",
    "\n",
    "**Reverse-mode autodiff (automatic differentiation)** is a mathematical technique for efficiently computing gradients in the context of machine learning and neural network training.\n",
    "\n",
    "1. **Forward Evaluation:**\n",
    "   - During the forward pass, the computational graph is constructed to represent the operations performed in the model.\n",
    "   - Intermediate values and operations are recorded during this phase.\n",
    "\n",
    "2. **Backward Differentiation:**\n",
    "   - In the backward pass, gradients are calculated by propagating the derivative of the loss backward through the computational graph.\n",
    "   - The chain rule is applied to compute the gradients efficiently.\n",
    "\n",
    "### Difference between Backpropagation and Reverse-Mode Autodiff:\n",
    "\n",
    "1. **Terminology:**\n",
    "   - \"Backpropagation\" often specifically refers to the training algorithm for neural networks, including both the forward and backward passes.\n",
    "   - \"Reverse-mode autodiff\" is a more general term that describes the automatic computation of derivatives in computational graphs, not limited to neural networks.\n",
    "\n",
    "2. **Scope:**\n",
    "   - Backpropagation is a specific application of reverse-mode autodiff in the context of neural network training.\n",
    "   - Reverse-mode autodiff is a broader concept used in various mathematical contexts, not exclusive to neural networks.\n",
    "\n",
    "In summary, backpropagation is a type of reverse-mode autodiff used for training neural networks. Reverse-mode autodiff, as a broader concept, can be applied in various mathematical contexts beyond neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the\n",
    "training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "\n",
    "In a Multilayer Perceptron (MLP), there are several hyperparameters that you can tweak to influence its training and performance. Here is a list of common hyperparameters in an MLP:\n",
    "\n",
    "1. **Number of Hidden Layers:**\n",
    "   - The number of layers between the input and output layers.\n",
    "\n",
    "2. **Number of Neurons in Each Hidden Layer:**\n",
    "   - The number of artificial neurons in each hidden layer.\n",
    "\n",
    "3. **Learning Rate:**\n",
    "   - Determines the step size during optimization. It influences how much the weights are adjusted during each iteration of training.\n",
    "\n",
    "4. **Activation Functions:**\n",
    "   - The activation function applied to the output of each neuron in the hidden layers. Common choices include ReLU, sigmoid, and tanh.\n",
    "\n",
    "5. **Weight Initialization:**\n",
    "   - The method used to initialize the weights before training. Common methods include random initialization, Xavier/Glorot initialization, and He initialization.\n",
    "\n",
    "6. **Regularization Techniques:**\n",
    "   - Techniques to prevent overfitting, such as L1 or L2 regularization, dropout, and early stopping.\n",
    "\n",
    "7. **Batch Size:**\n",
    "   - The number of training examples used in each iteration of training.\n",
    "\n",
    "8. **Number of Epochs:**\n",
    "   - The number of times the entire training dataset is passed through the network during training.\n",
    "\n",
    "9. **Optimizer:**\n",
    "   - The optimization algorithm used for updating weights during training. Common choices include stochastic gradient descent (SGD), Adam, RMSprop, etc.\n",
    "\n",
    "10. **Learning Rate Schedule:**\n",
    "    - A strategy for changing the learning rate during training. This can be a fixed schedule or adaptive methods.\n",
    "\n",
    "11. **Momentum:**\n",
    "    - A parameter that adds a fraction of the previous weight update to the current update during optimization.\n",
    "\n",
    "If an MLP is overfitting the training data, meaning it performs well on the training set but poorly on new, unseen data, you can try the following hyperparameter adjustments:\n",
    "\n",
    "1. **Reduce Model Complexity:**\n",
    "   - Decrease the number of hidden layers or neurons in each layer to reduce the model's capacity.\n",
    "\n",
    "2. **Apply Regularization:**\n",
    "   - Increase the strength of regularization techniques such as L1 or L2 regularization. Alternatively, add dropout to the hidden layers.\n",
    "\n",
    "3. **Adjust Learning Rate:**\n",
    "   - Experiment with different learning rates. A smaller learning rate might help prevent overfitting.\n",
    "\n",
    "4. **Early Stopping:**\n",
    "   - Monitor the performance on a validation set during training and stop training when the performance on the validation set starts to degrade.\n",
    "\n",
    "5. **Data Augmentation:**\n",
    "   - Introduce data augmentation techniques to artificially increase the size of the training dataset.\n",
    "\n",
    "6. **Batch Normalization:**\n",
    "   - Add batch normalization layers to normalize the inputs to hidden layers, which can help stabilize and accelerate training.\n",
    "\n",
    "7. **Hyperparameter Search:**\n",
    "   - Perform a systematic hyperparameter search, possibly using techniques like grid search or random search, to find the best combination of hyperparameters for your specific problem.\n",
    "\n",
    "It's important to note that the effectiveness of these adjustments may vary depending on the specific characteristics of your data and the problem at hand. Experimentation and monitoring the model's performance on a validation set are essential for finding the optimal set of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try\n",
    "adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of\n",
    "an interruption, add summaries, plot learning curves using TensorBoard, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rushabh.Patel2\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Rushabh.Patel2\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Rushabh.Patel2\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\Rushabh.Patel2\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Rushabh.Patel2\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.9000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rushabh.Patel2\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 19s 9ms/step - loss: 0.3290 - accuracy: 0.9001 - val_loss: 0.1213 - val_accuracy: 0.9640\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.1782 - accuracy: 0.9473 - val_loss: 0.0981 - val_accuracy: 0.9705\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.1483 - accuracy: 0.9560 - val_loss: 0.0809 - val_accuracy: 0.9755\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1304 - accuracy: 0.9605 - val_loss: 0.0862 - val_accuracy: 0.9761\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.1202 - accuracy: 0.9633 - val_loss: 0.0785 - val_accuracy: 0.9766\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1082 - accuracy: 0.9675 - val_loss: 0.0747 - val_accuracy: 0.9788\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1028 - accuracy: 0.9698 - val_loss: 0.0788 - val_accuracy: 0.9775\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0978 - accuracy: 0.9708 - val_loss: 0.0691 - val_accuracy: 0.9813\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0958 - accuracy: 0.9717 - val_loss: 0.0651 - val_accuracy: 0.9823\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0916 - accuracy: 0.9730 - val_loss: 0.0752 - val_accuracy: 0.9812\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0870 - accuracy: 0.9740 - val_loss: 0.0729 - val_accuracy: 0.9814\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0830 - accuracy: 0.9750 - val_loss: 0.0768 - val_accuracy: 0.9816\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0795 - accuracy: 0.9762 - val_loss: 0.0719 - val_accuracy: 0.9830\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0809 - accuracy: 0.9764 - val_loss: 0.0771 - val_accuracy: 0.9817\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0766 - accuracy: 0.9773 - val_loss: 0.0725 - val_accuracy: 0.9818\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0750 - accuracy: 0.9785 - val_loss: 0.0659 - val_accuracy: 0.9831\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0771 - accuracy: 0.9780 - val_loss: 0.0741 - val_accuracy: 0.9820\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0699 - accuracy: 0.9797 - val_loss: 0.0687 - val_accuracy: 0.9838\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0706 - accuracy: 0.9790 - val_loss: 0.0788 - val_accuracy: 0.9832\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0717 - accuracy: 0.9791 - val_loss: 0.0716 - val_accuracy: 0.9814\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0716 - accuracy: 0.9814\n",
      "Test Accuracy: 98.14%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to between 0 and 1\n",
    "\n",
    "# Flatten the images\n",
    "x_train_flat = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test_flat = x_test.reshape((x_test.shape[0], -1))\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "# Build the MLP model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"model_checkpoint.h5\", save_best_only=True)\n",
    "tensorboard_cb = TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x_train_flat, y_train_cat, epochs=20,\n",
    "    validation_data=(x_test_flat, y_test_cat),\n",
    "    callbacks=[checkpoint_cb, tensorboard_cb]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test_flat, y_test_cat)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
