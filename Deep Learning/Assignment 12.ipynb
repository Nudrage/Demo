{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How does unsqueeze help us to solve certain broadcasting problems?\n",
    "\n",
    "\n",
    "In NumPy, PyTorch, and other similar libraries, `unsqueeze` is a function that adds a new axis to a tensor. This operation is particularly useful when dealing with broadcasting problems, where the shapes of the involved tensors do not match, and broadcasting rules need to be applied. Broadcasting is a mechanism that allows element-wise operations between arrays of different shapes and sizes. The smaller array is \"broadcast\" across the larger array to match its shape, allowing the operation to be performed.\n",
    "\n",
    "Here's how `unsqueeze` can help in solving broadcasting problems:\n",
    "\n",
    "1. **Adding Dimensions:**\n",
    "   - Sometimes, when working with tensors of lower dimensions, you may need to perform operations with tensors of higher dimensions. `unsqueeze` allows you to add new dimensions to the tensor, making its shape compatible with the other tensor for broadcasting.\n",
    "   - Example:\n",
    "     ```python\n",
    "     import torch\n",
    "\n",
    "     # Suppose you have a tensor of shape (3,)\n",
    "     a = torch.tensor([1, 2, 3])\n",
    "\n",
    "     # You want to add it to a tensor of shape (3, 4)\n",
    "     b = torch.tensor([[10, 20, 30, 40],\n",
    "                       [50, 60, 70, 80],\n",
    "                       [90, 100, 110, 120]])\n",
    "\n",
    "     # You can unsqueeze the tensor 'a' to shape (3, 1) to make it compatible for broadcasting\n",
    "     result = a.unsqueeze(1) + b\n",
    "\n",
    "     print(result)\n",
    "     ```\n",
    "     In this example, `unsqueeze(1)` adds a new dimension to the tensor 'a,' changing its shape from (3,) to (3, 1). Now, it can be broadcasted across the tensor 'b.'\n",
    "\n",
    "2. **Matching Dimensions:**\n",
    "   - Broadcasting requires dimensions to be compatible. If two dimensions are equal or one of them is 1, broadcasting can occur. `unsqueeze` can be used to add dimensions with size 1 to make the shapes compatible.\n",
    "   - Example:\n",
    "     ```python\n",
    "     import torch\n",
    "\n",
    "     # Suppose you have a tensor of shape (2, 3)\n",
    "     a = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "\n",
    "     # You want to add it to a tensor of shape (1, 3)\n",
    "     b = torch.tensor([[10, 20, 30]])\n",
    "\n",
    "     # You can unsqueeze the tensor 'b' to shape (1, 3) to make it compatible for broadcasting\n",
    "     result = a + b.unsqueeze(0)\n",
    "\n",
    "     print(result)\n",
    "     ```\n",
    "     Here, `unsqueeze(0)` adds a new dimension with size 1 to the tensor 'b,' changing its shape from (3,) to (1, 3). Now, it can be broadcasted across the tensor 'a.'\n",
    "\n",
    "By using `unsqueeze` strategically, you can ensure that tensors have compatible shapes for broadcasting, making it easier to perform element-wise operations across tensors of different shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How can we use indexing to do the same operation as unsqueeze?\n",
    "\n",
    "\n",
    "Indexing can be used to achieve similar results as `unsqueeze` by manipulating the shape of a tensor. When using indexing, you can leverage the fact that broadcasting rules allow dimensions with size 1 to be implicitly expanded during operations. Here's how indexing can be used to achieve the same operation as `unsqueeze`:\n",
    "\n",
    "1. **Adding Dimensions:**\n",
    "   - If you want to add a new dimension to a tensor, you can use indexing with `None` (or `np.newaxis` in NumPy). This implicitly adds a new axis with size 1.\n",
    "   - Example (PyTorch):\n",
    "     ```python\n",
    "     import torch\n",
    "\n",
    "     # Suppose you have a tensor of shape (3,)\n",
    "     a = torch.tensor([1, 2, 3])\n",
    "\n",
    "     # You want to add it to a tensor of shape (3, 4)\n",
    "     b = torch.tensor([[10, 20, 30, 40],\n",
    "                       [50, 60, 70, 80],\n",
    "                       [90, 100, 110, 120]])\n",
    "\n",
    "     # You can use indexing to add a new axis to 'a' with size 1\n",
    "     result = a[:, None] + b\n",
    "\n",
    "     print(result)\n",
    "     ```\n",
    "     In this example, `a[:, None]` adds a new axis to the tensor 'a,' changing its shape from (3,) to (3, 1). Now, it can be broadcasted across the tensor 'b.'\n",
    "\n",
    "2. **Matching Dimensions:**\n",
    "   - If you want to match dimensions for broadcasting, you can use indexing with `None` to add dimensions with size 1 where needed.\n",
    "   - Example (PyTorch):\n",
    "     ```python\n",
    "     import torch\n",
    "\n",
    "     # Suppose you have a tensor of shape (2, 3)\n",
    "     a = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "\n",
    "     # You want to add it to a tensor of shape (1, 3)\n",
    "     b = torch.tensor([[10, 20, 30]])\n",
    "\n",
    "     # You can use indexing to add a new axis to 'b' with size 1\n",
    "     result = a + b[None, :]\n",
    "\n",
    "     print(result)\n",
    "     ```\n",
    "     Here, `b[None, :]` adds a new axis to the tensor 'b,' changing its shape from (3,) to (1, 3). Now, it can be broadcasted across the tensor 'a.'\n",
    "\n",
    "Both of these indexing techniques achieve results similar to using `unsqueeze` and can be useful when you need more control over the shape of the tensors for broadcasting operations. Choose the approach that fits your coding style and preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How do we show the actual contents of the memory used for a tensor?\n",
    "\n",
    "To inspect the actual contents of the memory used for a tensor in Python, you can use the `.numpy()` method to convert the tensor to a NumPy array, and then print the array. This will reveal the values stored in the memory. Here's an example using PyTorch:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Convert the tensor to a NumPy array and print\n",
    "numpy_array = tensor_a.numpy()\n",
    "print(numpy_array)\n",
    "```\n",
    "\n",
    "In this example, `tensor_a` is converted to a NumPy array using `.numpy()`, and then the NumPy array is printed. The values in the memory are displayed as a NumPy array.\n",
    "\n",
    "If you want to inspect the memory contents of a tensor in more detail, you can use the `torch.Tensor.data_ptr()` method to obtain the pointer to the data. This pointer can be used to access the raw memory:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Get the pointer to the data\n",
    "data_ptr = tensor_a.data_ptr()\n",
    "\n",
    "# Print the content of the memory\n",
    "memory_content = torch.tensor([torch.load(data_ptr + i) for i in range(tensor_a.numel())])\n",
    "print(memory_content)\n",
    "```\n",
    "\n",
    "Keep in mind that directly inspecting the memory content like this is not a typical operation and should be done with caution. The `numpy()` method is generally more convenient for viewing the tensor values, while the second example with `data_ptr()` is more of a low-level approach for understanding the underlying memory structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)\n",
    "\n",
    "When adding a vector of size 3 to a matrix of size 3×3, the vector is broadcasted along each row of the matrix. This means that the elements of the vector are added to each corresponding element in the rows of the matrix. Let's demonstrate this with a simple example using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Vector:\n",
      "[10 20 30]\n",
      "\n",
      "Result after adding the vector to each row:\n",
      "[[11 22 33]\n",
      " [14 25 36]\n",
      " [17 28 39]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a 3x3 matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])\n",
    "\n",
    "# Create a vector of size 3\n",
    "vector = np.array([10, 20, 30])\n",
    "\n",
    "# Add the vector to each row of the matrix\n",
    "result = matrix + vector\n",
    "\n",
    "print(\"Original Matrix:\")\n",
    "print(matrix)\n",
    "\n",
    "print(\"\\nVector:\")\n",
    "print(vector)\n",
    "\n",
    "print(\"\\nResult after adding the vector to each row:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "\n",
    "Broadcasting and `expand_as` in PyTorch are designed to efficiently handle operations without significantly increasing memory usage. These mechanisms aim to perform operations without explicitly creating multiple copies of the data, making them memory-efficient.\n",
    "\n",
    "Let's discuss each of these concepts:\n",
    "\n",
    "#### Broadcasting:\n",
    "Broadcasting allows element-wise operations on arrays with different shapes. It avoids creating multiple copies of the data by virtually expanding the smaller array to match the shape of the larger array. The expanded array is not physically duplicated in memory; instead, the operations are performed as if the array were replicated along the required dimensions.\n",
    "\n",
    "#### `expand_as`:\n",
    "The `expand_as` method in PyTorch is used to expand the size of one tensor to match the size of another tensor. Similar to broadcasting, `expand_as` does not create a new tensor with duplicated data. It merely returns a view of the original tensor with expanded dimensions. The memory efficiency is maintained because the expanded tensor shares the same underlying data with the original tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Implement matmul using Einstein summation.\n",
    "\n",
    "Einstein summation notation provides a concise and expressive way to perform various operations on tensors. We can use Einstein summation to implement matrix multiplication (`matmul`). Here's an example of how you can implement `matmul` using Einstein summation in Python with NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "Matrix B:\n",
      "[[ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]]\n",
      "\n",
      "Result of matmul:\n",
      "[[ 58  64]\n",
      " [139 154]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def matmul(A, B):\n",
    "    # Ensure that the number of columns in A matches the number of rows in B\n",
    "    assert A.shape[1] == B.shape[0], \"Incompatible shapes for matrix multiplication\"\n",
    "\n",
    "    # Einstein summation for matrix multiplication\n",
    "    result = np.einsum('ij,jk->ik', A, B)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example matrices\n",
    "matrix_A = np.array([[1, 2, 3],\n",
    "                    [4, 5, 6]])\n",
    "\n",
    "matrix_B = np.array([[7, 8],\n",
    "                    [9, 10],\n",
    "                    [11, 12]])\n",
    "\n",
    "# Perform matrix multiplication using the implemented matmul\n",
    "result = matmul(matrix_A, matrix_B)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(matrix_A)\n",
    "\n",
    "print(\"\\nMatrix B:\")\n",
    "print(matrix_B)\n",
    "\n",
    "print(\"\\nResult of matmul:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What does a repeated index letter represent on the lefthand side of einsum?\n",
    "\n",
    "\n",
    "In Einstein summation notation, a repeated index letter on the left-hand side of `einsum` indicates summation or contraction along that index. The notation is used to specify how the indices of input arrays should be combined to produce the output array.\n",
    "\n",
    "Here's a breakdown of what a repeated index letter represents:\n",
    "\n",
    "- **No Repeated Index:**\n",
    "  - If an index letter appears once, it means that the dimensions along that index are aligned between the input arrays. The output array will have the same dimensions along that index.\n",
    "\n",
    "- **Repeated Index:**\n",
    "  - If an index letter appears more than once, it indicates summation or contraction along that index. The output array will have reduced dimensions along that index, and the values will be summed over.\n",
    "\n",
    "**Example:**\n",
    "Consider the Einstein summation notation `'ij,jk->ik'`. In this notation:\n",
    "\n",
    "- `i` is an index that appears twice (once in the first term, and once in the second term).\n",
    "- `j` is an index that appears once in the first term.\n",
    "- `k` is an index that appears once in the second term.\n",
    "\n",
    "The resulting output array will have dimensions along `i` and `k`, and the summation will be performed over the repeated index `j`. The notation specifies the contraction of indices `j` in the first term and `j` in the second term.\n",
    "\n",
    "Here's how the notation corresponds to a matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "Matrix B:\n",
      "[[5 6]\n",
      " [7 8]]\n",
      "\n",
      "Result of einsum ('ij,jk->ik'):\n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example matrices\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "# Einstein summation for matrix multiplication: 'ij,jk->ik'\n",
    "result = np.einsum('ij,jk->ik', A, B)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "\n",
    "print(\"\\nResult of einsum ('ij,jk->ik'):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. What are the three rules of Einstein summation notation? Why?\n",
    "\n",
    "Einstein summation notation follows three fundamental rules that govern how indices are manipulated when performing tensor operations. These rules help express complex operations more concisely. The three rules of Einstein summation notation are:\n",
    "\n",
    "1. **Repeating Indices Imply Summation:**\n",
    "   - **Rule:** If an index appears twice in a product term (once as a subscript and once as a superscript), it implies summation over that index.\n",
    "   - **Example:** In the expression \\($A_{ij}B_{jk}$\\), the index \\($j$\\) appears twice, implying summation over \\($j$\\).\n",
    "\n",
    "2. **Unmatched Indices Indicate Independent Variables:**\n",
    "   - **Rule:** If an index appears exactly once as a subscript and exactly once as a superscript in a product term, it indicates an independent variable.\n",
    "   - **Example:** In the expression \\($A_{ij}B^{ij}$\\), each index \\($i$\\) appears once as a subscript and once as a superscript, indicating an independent variable.\n",
    "\n",
    "3. **Indices Only Sum Over If Repeated:**\n",
    "   - **Rule:** An index is summed over only if it is repeated in a product term. If an index appears once as a subscript and once as a superscript in a product term, it does not imply summation.\n",
    "   - **Example:** In the expression \\($A_{ij}B^{ij}C_{k}$\\), the indices \\($i$\\) and \\($j$\\) are repeated, so summation occurs. However, the index \\($k$\\) appears once as a subscript and once as a superscript, indicating an independent variable.\n",
    "\n",
    "**Why these Rules:**\n",
    "Einstein summation notation is designed to provide a concise representation of tensor operations. The rules reflect the fundamental operations of contraction and element-wise multiplication that are common in linear algebra and tensor calculus. The notation simplifies complex expressions by removing the need for explicit summation symbols and indices, making it easier to express and understand tensor operations. Additionally, the rules help ensure that the dimensions of tensors are appropriately matched in the resulting expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. What are the forward pass and backward pass of a neural network?\n",
    "\n",
    "\n",
    "The forward pass and backward pass are two key phases in the training of a neural network, specifically during the process of backpropagation, which is used to update the network's parameters (weights and biases) based on the error or loss. These phases are integral to the training process of supervised learning in neural networks.\n",
    "\n",
    "#### Forward Pass:\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - The process begins with passing the input data through the input layer of the neural network.\n",
    "\n",
    "2. **Hidden Layers:**\n",
    "   - The input data is then propagated through the hidden layers of the network. Each neuron in a layer performs a weighted sum of its inputs, adds a bias term, and applies an activation function to produce the output.\n",
    "\n",
    "3. **Output Layer:**\n",
    "   - The process continues through the hidden layers until the output layer is reached. The output layer produces the predicted output or scores for the given input.\n",
    "\n",
    "#### Backward Pass:\n",
    "\n",
    "1. **Gradient Calculation:**\n",
    "   - The backward pass starts with computing the gradients of the loss with respect to the parameters (weights and biases) of the network. This is done using the chain rule of calculus.\n",
    "\n",
    "2. **Backpropagation:**\n",
    "   - The gradients are propagated backward through the network using a technique called backpropagation. The backpropagation algorithm calculates the gradients layer by layer, updating the parameters to minimize the loss.\n",
    "\n",
    "3. **Parameter Update:**\n",
    "   - The parameters are updated using an optimization algorithm (e.g., gradient descent, Adam). The goal is to adjust the weights and biases in a way that reduces the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
    "\n",
    "\n",
    "1. **Activations as Inputs:**\n",
    "   - The activations of intermediate layers serve as inputs to subsequent layers. Storing these activations is necessary to compute gradients during backpropagation.\n",
    "\n",
    "2. **Efficiency and Memory:**\n",
    "   - Storing activations during the forward pass avoids the need to recalculate them during the backward pass, which can be computationally expensive. It also saves memory, especially when activations are used in multiple places during backpropagation.\n",
    "\n",
    "3. **Non-linearity Preservation:**\n",
    "   - Some activation functions (e.g., ReLU) discard negative values. Storing activations ensures that the original pre-activation values are available during backpropagation, preserving information for gradient calculations.\n",
    "\n",
    "In summary, storing activations during the forward pass is essential for efficiently computing gradients during the backward pass. It facilitates the application of the chain rule and enables the efficient training of neural networks through backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "\n",
    "\n",
    "Having activations with a standard deviation too far away from 1 in a neural network can lead to issues during training. The choice of appropriate activation function and weight initialization plays a crucial role in maintaining stable and effective training. Here are the downsides of activations with a standard deviation significantly deviating from 1:\n",
    "\n",
    "1. **Vanishing or Exploding Gradients:**\n",
    "   - If the standard deviation of activations is too small, it may lead to vanishing gradients during backpropagation. This occurs when gradients become very close to zero, and the network struggles to update the weights in earlier layers, impeding learning.\n",
    "   - Conversely, if the standard deviation is too large, it can result in exploding gradients. Gradients become very large, causing weight updates that are too drastic and can lead to convergence issues.\n",
    "\n",
    "2. **Saturated Activation Regions:**\n",
    "   - Some activation functions, such as the sigmoid and hyperbolic tangent (tanh), saturate for extreme input values, resulting in very small gradients. If the activations are too far from 1, these saturation issues may occur, slowing down or hindering learning.\n",
    "\n",
    "3. **Slow Convergence:**\n",
    "   - Activations with a standard deviation significantly different from 1 can slow down the convergence of the training process. This is because learning rates and weight updates may need to be adjusted to compensate for the scaling of the activations.\n",
    "\n",
    "4. **Unstable Training Dynamics:**\n",
    "   - Extreme standard deviations in activations can lead to unstable training dynamics. The network may oscillate between saturation and non-saturation regions, making it challenging to find an optimal set of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. How can weight initialization help avoid this problem?\n",
    "\n",
    "\n",
    "Weight initialization is a crucial factor in training neural networks and can help avoid issues such as vanishing or exploding gradients during the training process. Properly initializing the weights of a neural network contributes to more stable and efficient learning. Here's how weight initialization helps mitigate these problems:\n",
    "\n",
    "1. **Vanishing Gradients:**\n",
    "   - When weights are initialized too small, activations in deep networks can become very small, leading to vanishing gradients during backpropagation. In the early layers, the gradients may approach zero, hindering weight updates and slowing down learning.\n",
    "   - Weight initialization methods that set initial weights to values suitable for the scale of the activation function can prevent vanishing gradients. For example, He initialization is often used with ReLU activation, and Xavier/Glorot initialization is suitable for tanh or sigmoid activations.\n",
    "\n",
    "2. **Exploding Gradients:**\n",
    "   - Conversely, if weights are initialized too large, the gradients during backpropagation can become very large, leading to exploding gradients. This can cause weight updates that are too drastic and result in convergence issues.\n",
    "   - Weight initialization methods that take into account the number of input and output units of a layer, like He initialization or Xavier/Glorot initialization, help in controlling the scale of the gradients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
