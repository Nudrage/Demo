{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.\tWhat are the advantages of a CNN over a fully connected DNN for image classification?\n",
    "\n",
    "Convolutional Neural Networks (CNNs) offer several advantages over fully connected Deep Neural Networks (DNNs) for image classification tasks:\n",
    "\n",
    "1. **Spatial Hierarchical Structure:** CNNs are designed to recognize patterns with a spatial hierarchy. They preserve the spatial structure of the image through the use of convolutional layers, which are better suited for capturing local patterns like edges, textures, and shapes.\n",
    "\n",
    "2. **Parameter Sharing:** In CNNs, the parameters of the convolutional filters are shared across the entire image. This significantly reduces the number of parameters compared to fully connected DNNs, making CNNs more efficient and easier to train, especially for large images.\n",
    "\n",
    "3. **Translation Invariance:** CNNs exploit the property of translation invariance, meaning they can recognize patterns regardless of their position in the image. This is achieved through the use of convolutional and pooling layers, which downsample the input while preserving important features.\n",
    "\n",
    "4. **Feature Hierarchies:** CNNs automatically learn hierarchical representations of features from raw pixel values. The early layers capture simple features like edges and corners, while deeper layers learn more abstract and complex features. This hierarchical approach allows CNNs to capture high-level semantic information, which is crucial for tasks like image classification.\n",
    "\n",
    "5. **Local Connectivity:** CNNs take advantage of local connectivity, meaning each neuron is only connected to a small region of the input volume. This local connectivity helps reduce the computational complexity of the network while maintaining the ability to capture spatial dependencies within the image.\n",
    "\n",
    "6. **Regularization:** CNN architectures often include techniques like dropout and batch normalization, which help prevent overfitting and improve generalization performance. These techniques are particularly effective in CNNs due to their hierarchical structure and shared parameterization.\n",
    "\n",
    "7. **Memory Efficiency:** CNNs require less memory compared to fully connected DNNs because they operate on smaller patches of the input image at a time, rather than processing the entire image as a single vector. This makes CNNs more practical for handling large images or datasets with limited computational resources.\n",
    "\n",
    "Overall, these advantages make CNNs the preferred choice for image classification tasks, as they are specifically tailored to exploit the spatial structure and hierarchical nature of images, leading to more accurate and efficient models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.\tConsider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?\n",
    "\n",
    "To calculate the total number of parameters in the CNN, we need to consider the parameters for each layer:\n",
    "\n",
    "1. **First Convolutional Layer:**\n",
    "   - Input: RGB image of size 200x300 pixels (3 channels)\n",
    "   - Output: 100 feature maps\n",
    "   - Each feature map has a 3x3 kernel, and there are 100 of them.\n",
    "   - Parameters = (3 * 3 * 3) * 100 + 100 (biases)\n",
    "                = 2700 * 100 + 100\n",
    "                = 270,100 parameters\n",
    "\n",
    "2. **Second Convolutional Layer:**\n",
    "   - Input: 100 feature maps\n",
    "   - Output: 200 feature maps\n",
    "   - Each feature map has a 3x3 kernel, and there are 200 of them.\n",
    "   - Parameters = (3 * 3 * 100) * 200 + 200\n",
    "                = 2700 * 200 + 200\n",
    "                = 540,200 parameters\n",
    "\n",
    "3. **Third Convolutional Layer:**\n",
    "   - Input: 200 feature maps\n",
    "   - Output: 400 feature maps\n",
    "   - Each feature map has a 3x3 kernel, and there are 400 of them.\n",
    "   - Parameters = (3 * 3 * 200) * 400 + 400\n",
    "                = 5400 * 400 + 400\n",
    "                = 2,161,200 parameters\n",
    "\n",
    "Total number of parameters in the CNN:\n",
    "   = 270,100 + 540,200 + 2,161,200\n",
    "   = 2,971,500 parameters\n",
    "\n",
    "To estimate the RAM required, we also need to consider the size of the input image and the size of the feature maps in each layer:\n",
    "\n",
    "- Input image size: 200x300 pixels * 3 channels * 32 bits/pixel (for 32-bit floats)\n",
    "                    = 200 * 300 * 3 * 4 bytes\n",
    "                    = 720,000 bytes (or 0.72 MB)\n",
    "\n",
    "Now, let's calculate the RAM required for prediction and training:\n",
    "\n",
    "**For prediction (for a single instance):**\n",
    "- We only need to store the input image and the feature maps of each layer (as we don't need to store gradients during inference).\n",
    "- Total RAM required = Input image size + Sum of feature maps sizes\n",
    "\n",
    "**For training (for a mini-batch of 50 images):**\n",
    "- In addition to the input images and feature maps, we need to store activations, gradients, and other variables for each layer, which would increase the memory requirement significantly.\n",
    "\n",
    "Without specific information about the activation size and other variables, it's challenging to provide an accurate estimate for the RAM required during training. However, it would be considerably larger than during inference due to the need to store additional information for backpropagation.\n",
    "\n",
    "So, for prediction:\n",
    "- Total RAM required ≈ 0.72 MB + Size of feature maps from each layer\n",
    "\n",
    "And for training, it would be significantly larger, but the exact calculation would require more information about the network's architecture and implementation details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.\tIf your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?\n",
    "\n",
    "If your GPU runs out of memory while training a CNN, there are several steps you can take to address the issue:\n",
    "\n",
    "1. **Reduce Batch Size:** Decrease the batch size used during training. A smaller batch size requires less memory to store activations and gradients during forward and backward passes. However, reducing the batch size may also affect the convergence speed and the quality of the model's updates.\n",
    "\n",
    "2. **Decrease Model Complexity:** Simplify the architecture of your CNN by reducing the number of layers, decreasing the number of feature maps in each layer, or using smaller kernel sizes. This reduces the number of parameters and activations, thereby requiring less memory.\n",
    "\n",
    "3. **Use Mixed Precision Training:** Utilize mixed precision training, which involves using 16-bit floating-point (half-precision) numbers instead of 32-bit floating-point (single-precision) numbers to represent weights, activations, and gradients. This reduces the memory footprint while training, as each value occupies half the memory compared to single-precision numbers. However, it may require careful adjustment of the training process to maintain numerical stability.\n",
    "\n",
    "4. **Data Augmentation:** Implement data augmentation techniques to generate additional training samples on-the-fly. Data augmentation helps improve the generalization ability of the model and reduces overfitting. By generating augmented samples dynamically during training, you can reduce the size of the training dataset stored in memory.\n",
    "\n",
    "5. **Gradient Checkpointing:** Implement gradient checkpointing techniques, which involve recomputing activations during the backward pass rather than storing them all in memory. This reduces the memory footprint at the expense of additional computation. Gradient checkpointing can be particularly useful when memory constraints prevent the entire computational graph from fitting into GPU memory.\n",
    "\n",
    "6. **Use a Larger GPU or Distributed Training:** If possible, train your CNN on a GPU with more memory or distribute the training process across multiple GPUs or machines. Distributed training allows you to parallelize the computation and distribute the memory usage across multiple devices, mitigating memory constraints. However, this may require additional hardware resources and changes to the training setup.\n",
    "\n",
    "By applying one or more of these techniques, you can often alleviate memory constraints and continue training your CNN effectively even when GPU memory is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.\tWhy would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n",
    "\n",
    "Adding a max pooling layer instead of a convolutional layer with the same stride serves several purposes and brings about distinct advantages:\n",
    "\n",
    "1. **Dimensionality Reduction:** Max pooling reduces the spatial dimensions of the feature maps. This reduction helps in decreasing the computational complexity of the network by reducing the number of parameters and operations in subsequent layers. In contrast, a convolutional layer with the same stride maintains the same spatial dimensions, potentially increasing the computational load.\n",
    "\n",
    "2. **Translation Invariance:** Max pooling introduces a form of translation invariance by retaining the maximum activation within each pooling region. This means that the exact location of the features becomes less relevant, which can improve the network's robustness to small spatial translations or distortions in the input.\n",
    "\n",
    "3. **Increased Receptive Field:** Max pooling effectively increases the receptive field of the network without introducing additional parameters. By reducing the spatial dimensions of the feature maps, max pooling allows subsequent layers to have a larger view of the input image, which can be beneficial for capturing more global features and improving the network's discriminative power.\n",
    "\n",
    "4. **Feature Selection:** Max pooling selects the most important features within each pooling region by retaining only the maximum activation. This helps in discarding irrelevant or redundant information and emphasizing the most salient features, which can lead to more discriminative representations.\n",
    "\n",
    "5. **Regularization:** Max pooling acts as a form of regularization by introducing a form of spatial aggregation and local feature selection. This can help prevent overfitting by reducing the capacity of the network and promoting the learning of more robust and generalizable features.\n",
    "\n",
    "Overall, while a convolutional layer with the same stride maintains spatial dimensions and captures local spatial patterns, adding a max pooling layer provides benefits such as dimensionality reduction, translation invariance, increased receptive field, feature selection, and regularization, making it a valuable component in CNN architectures for various computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.\tWhen would you want to add a local response normalization layer?\n",
    "\n",
    "Local Response Normalization (LRN) layers are typically added to Convolutional Neural Networks (CNNs) for specific reasons, often related to improving generalization performance and reducing overfitting. Here are some scenarios where adding an LRN layer might be beneficial:\n",
    "\n",
    "1. **Improving Generalization:** LRN layers can help improve the generalization performance of a CNN by promoting competition among neighboring neurons. By normalizing the activations within a local neighborhood, LRN layers encourage neurons to respond to relatively large, spatially distant features rather than focusing solely on localized features. This can lead to better generalization to unseen data by encouraging the network to learn more robust and discriminative features.\n",
    "\n",
    "2. **Mitigating Overfitting:** LRN layers act as a form of regularization by normalizing activations within local neighborhoods. This can help prevent neurons from becoming overly sensitive to specific input patterns present in the training data, thereby reducing the risk of overfitting. By encouraging more diverse and generalized feature representations, LRN layers can help improve the model's ability to generalize to new, unseen data.\n",
    "\n",
    "3. **Enhancing Discriminative Power:** LRN layers can enhance the discriminative power of the learned features by emphasizing relatively strong activations and suppressing weaker activations within local neighborhoods. This can help highlight important features while suppressing noise or irrelevant information, leading to more effective feature representations and better overall performance on classification tasks.\n",
    "\n",
    "4. **Enhancing Local Contrast:** LRN layers can enhance the local contrast of feature maps by normalizing activations within local neighborhoods. This can help sharpen the boundaries between different objects or regions in the input images, making it easier for subsequent layers to extract meaningful features and make accurate predictions.\n",
    "\n",
    "5. **Complementing Other Normalization Techniques:** LRN layers can be used in conjunction with other normalization techniques, such as batch normalization or instance normalization, to further enhance the stability and performance of the network. By providing different forms of normalization at different stages of the network, it's possible to achieve improved performance and stability across a wide range of tasks and datasets.\n",
    "\n",
    "Overall, LRN layers can be beneficial in CNN architectures for improving generalization performance, reducing overfitting, enhancing discriminative power, enhancing local contrast, and complementing other normalization techniques. However, their effectiveness may vary depending on the specific task, dataset, and architecture, so it's essential to experiment and evaluate their impact empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.\tCan you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?\n",
    "\n",
    "Certainly! Here's a summary of the main innovations introduced by each of the mentioned architectures compared to their predecessors:\n",
    "\n",
    "### AlexNet (Compared to LeNet-5):\n",
    "\n",
    "1. **Deeper Architecture:** AlexNet introduced a much deeper neural network architecture compared to LeNet-5. It consisted of eight layers, including five convolutional layers and three fully connected layers.\n",
    "\n",
    "2. **ReLU Activation:** AlexNet replaced the sigmoid activation function used in LeNet-5 with the Rectified Linear Unit (ReLU) activation function. ReLU helped mitigate the vanishing gradient problem and accelerated the training process by enabling faster convergence.\n",
    "\n",
    "3. **GPU Acceleration:** AlexNet was one of the first CNN architectures to leverage the power of Graphics Processing Units (GPUs) for training. The use of GPUs significantly accelerated the computation, making it feasible to train deeper networks efficiently.\n",
    "\n",
    "4. **Local Response Normalization (LRN):** AlexNet utilized LRN layers to normalize the activations within local neighborhoods, promoting competition between adjacent neurons and improving generalization performance.\n",
    "\n",
    "### GoogLeNet:\n",
    "\n",
    "1. **Inception Module:** GoogLeNet introduced the Inception module, which employs parallel convolutional operations with different kernel sizes and pooling operations. This architecture efficiently captures features at various scales without significantly increasing the computational cost.\n",
    "\n",
    "2. **Global Average Pooling:** Instead of fully connected layers at the top, GoogLeNet employed global average pooling to reduce the spatial dimensions of the feature maps directly before the final softmax layer. This reduced overfitting and the number of parameters in the network.\n",
    "\n",
    "### ResNet:\n",
    "\n",
    "1. **Residual Connections:** ResNet introduced residual connections, also known as skip connections, to enable the training of very deep networks (hundreds of layers) effectively. These connections allowed the gradient to flow more easily through the network, mitigating the vanishing gradient problem and facilitating the training of extremely deep architectures.\n",
    "\n",
    "### SENet (Squeeze-and-Excitation Networks):\n",
    "\n",
    "1. **SE Blocks:** SENet introduced Squeeze-and-Excitation (SE) blocks, which adaptively recalibrate channel-wise feature responses by explicitly modeling interdependencies between channels. SE blocks capture the dependencies between channels and improve feature discriminability, leading to better performance.\n",
    "\n",
    "### Xception:\n",
    "\n",
    "1. **Depthwise Separable Convolutions:** Xception replaced standard convolutions with depthwise separable convolutions, which consist of depthwise convolutions followed by pointwise convolutions. This architecture reduces the number of parameters and computations while maintaining expressive power, leading to more efficient and lightweight models.\n",
    "\n",
    "Each of these architectures introduced significant innovations that have had a profound impact on the field of deep learning, enabling advancements in various computer vision tasks such as image classification, object detection, and semantic segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7.\tWhat is a fully convolutional network? How can you convert a dense layer into a convolutional layer?\n",
    "\n",
    "A Fully Convolutional Network (FCN) is a type of neural network architecture that is designed for semantic segmentation tasks, where the goal is to classify each pixel in an image into different classes. FCNs are composed entirely of convolutional layers and do not contain any fully connected layers, hence the name \"fully convolutional.\"\n",
    "\n",
    "### Characteristics of Fully Convolutional Networks:\n",
    "\n",
    "1. **No Fully Connected Layers:** FCNs replace the fully connected layers typically found at the end of traditional CNN architectures with convolutional layers. This allows FCNs to accept input images of any size and produce output feature maps with spatial dimensions that match the input size.\n",
    "\n",
    "2. **Upsampling Layers:** FCNs incorporate upsampling layers, such as transposed convolutions or nearest-neighbor upsampling followed by convolutions, to increase the spatial resolution of the feature maps and produce pixel-wise predictions.\n",
    "\n",
    "3. **Skip Connections:** Many FCN architectures also incorporate skip connections, which connect feature maps from earlier layers to later layers. These connections help preserve fine-grained spatial information and improve the quality of segmentation masks.\n",
    "\n",
    "### Converting a Dense Layer into a Convolutional Layer:\n",
    "\n",
    "To convert a dense layer into a convolutional layer, you need to ensure that the resulting convolutional layer has the same functionality as the dense layer while preserving the spatial dimensions of the input. This can be achieved by using a convolutional layer with an appropriate kernel size and stride.\n",
    "\n",
    "Here's how you can convert a dense layer into a convolutional layer:\n",
    "\n",
    "1. **Reshape the Weights:** Reshape the weights of the dense layer into a 4D tensor with dimensions (kernel_height, kernel_width, input_channels, output_channels). The input_channels would correspond to the size of the input feature maps, while the output_channels would correspond to the number of neurons in the dense layer.\n",
    "\n",
    "2. **Apply Convolution:** Apply a convolution operation with the reshaped weights. Choose appropriate padding to ensure that the spatial dimensions of the input feature maps are preserved.\n",
    "\n",
    "3. **Adjust Stride and Padding:** Depending on the desired output size, adjust the stride and padding of the convolutional layer to achieve the desired spatial dimensions of the output feature maps.\n",
    "\n",
    "By following these steps, you can convert a dense layer into a convolutional layer, allowing you to incorporate it into a fully convolutional network architecture for tasks such as semantic segmentation or image-to-image translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8.\tWhat is the main technical difficulty of semantic segmentation?\n",
    "\n",
    "The main technical difficulty of semantic segmentation lies in accurately delineating and classifying objects or regions within an image at the pixel level. This task is challenging due to several factors:\n",
    "\n",
    "1. **Fine-Grained Localization:** Semantic segmentation requires precise localization of object boundaries and fine-grained delineation of object shapes within an image. Achieving accurate pixel-level segmentation necessitates capturing intricate details and subtle variations in texture, color, and shape.\n",
    "\n",
    "2. **Semantic Understanding:** Semantic segmentation involves not only distinguishing between different object categories but also understanding the semantics and context of each object instance within the scene. This requires the model to discern between objects of the same category (e.g., differentiating between various types of vehicles or animals) and accurately segmenting each instance separately.\n",
    "\n",
    "3. **Object Occlusion and Ambiguity:** Objects in real-world scenes often overlap or occlude each other, making it challenging to precisely delineate their boundaries. Additionally, objects may exhibit variability in appearance, pose, scale, and illumination conditions, further complicating the segmentation task.\n",
    "\n",
    "4. **Class Imbalance and Data Variability:** Semantic segmentation datasets typically exhibit class imbalance, where some object categories are more prevalent than others. Moreover, the variability in object appearance, viewpoint, and environmental conditions across different scenes introduces challenges in generalizing the segmentation model to unseen data.\n",
    "\n",
    "5. **Computational Complexity:** Pixel-wise segmentation involves processing high-resolution images with millions of pixels, resulting in a large computational burden. Efficiently handling this computational complexity while maintaining real-time performance is crucial for practical deployment of semantic segmentation models in applications such as autonomous driving, medical imaging, and augmented reality.\n",
    "\n",
    "Addressing these technical challenges requires the development of sophisticated deep learning architectures, innovative training strategies, effective data augmentation techniques, and robust evaluation metrics tailored specifically for semantic segmentation tasks. Moreover, ongoing research efforts focus on integrating contextual information, leveraging multi-scale features, and incorporating domain-specific knowledge to improve the accuracy and robustness of semantic segmentation models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9.\tBuild your own CNN from scratch and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 11s 28ms/step - loss: 0.2983 - accuracy: 0.9111 - val_loss: 0.0840 - val_accuracy: 0.9753\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 0.0711 - accuracy: 0.9785 - val_loss: 0.0611 - val_accuracy: 0.9805\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.0494 - accuracy: 0.9849 - val_loss: 0.0476 - val_accuracy: 0.9866\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.0359 - accuracy: 0.9889 - val_loss: 0.0456 - val_accuracy: 0.9861\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 11s 29ms/step - loss: 0.0317 - accuracy: 0.9898 - val_loss: 0.0486 - val_accuracy: 0.9840\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.0254 - accuracy: 0.9919 - val_loss: 0.0409 - val_accuracy: 0.9880\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.0339 - val_accuracy: 0.9898\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.0181 - accuracy: 0.9940 - val_loss: 0.0404 - val_accuracy: 0.9893\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 11s 28ms/step - loss: 0.0151 - accuracy: 0.9950 - val_loss: 0.0396 - val_accuracy: 0.9895\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0367 - val_accuracy: 0.9900\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.0271 - accuracy: 0.9906\n",
      "Test Accuracy: 0.9905999898910522\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test  = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test  = to_categorical(y_test, 10)\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
