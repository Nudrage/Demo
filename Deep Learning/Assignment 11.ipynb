{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write the Python code to implement a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0.72916074]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SingleNeuron:\n",
    "    def __init__(self, input_size, activation_function):\n",
    "        # Initialize weights and bias randomly\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.bias = np.random.rand(1)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate weighted sum and apply activation function\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "        output = self.activation_function(weighted_sum)\n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "# Define an activation function (e.g., sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Create a single neuron with 3 input features and the sigmoid activation function\n",
    "neuron = SingleNeuron(input_size=3, activation_function=sigmoid)\n",
    "\n",
    "# Input features\n",
    "inputs = np.array([0.5, -0.2, 0.1])\n",
    "\n",
    "# Forward pass through the neuron\n",
    "output = neuron.forward(inputs)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write the Python code to implement ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Values: [-2 -1  0  1  2]\n",
      "Output Values (ReLU): [0 0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        # Apply ReLU activation element-wise\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "# Example usage:\n",
    "# Create an instance of the ReLU activation function\n",
    "relu_activation = ReLU()\n",
    "\n",
    "# Input values\n",
    "input_values = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "# Apply ReLU activation to the input values\n",
    "output_values = relu_activation.forward(input_values)\n",
    "\n",
    "# Print the output\n",
    "print(\"Input Values:\", input_values)\n",
    "print(\"Output Values (ReLU):\", output_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write the Python code for a dense layer in terms of matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0.57959155 0.64617456]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size, activation_function):\n",
    "        # Initialize weights and bias randomly\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.bias = np.random.rand(output_size)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate weighted sum and apply activation function\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "        output = self.activation_function(weighted_sum)\n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "# Define an activation function (e.g., sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Create a dense layer with 3 input features, 2 output features, and the sigmoid activation function\n",
    "dense_layer = DenseLayer(input_size=3, output_size=2, activation_function=sigmoid)\n",
    "\n",
    "# Input features\n",
    "inputs = np.array([0.5, -0.2, 0.1])\n",
    "\n",
    "# Forward pass through the dense layer\n",
    "output = dense_layer.forward(inputs)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Dense Layer: [[0.51435917 0.63221935]\n",
      " [0.5493744  0.66428319]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size, activation_function):\n",
    "        # Initialize weights and bias with random values\n",
    "        self.weights = np.random.rand(output_size, input_size)\n",
    "        self.bias = np.random.rand(output_size, 1)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the weighted sum and apply the activation function\n",
    "        weighted_sum = np.dot(self.weights, inputs) + self.bias\n",
    "        output = self.activation_function(weighted_sum)\n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "# Define an activation function (e.g., sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Create a dense layer with 3 input neurons and 2 output neurons using sigmoid activation\n",
    "dense_layer = DenseLayer(input_size=3, output_size=2, activation_function=sigmoid)\n",
    "\n",
    "# Input features\n",
    "inputs = np.array([0.5, -0.2, 0.1])\n",
    "\n",
    "# Forward pass through the dense layer\n",
    "output = dense_layer.forward(inputs)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output of Dense Layer:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What is the “hidden size” of a layer?\n",
    "\n",
    "\n",
    "The \"hidden size\" of a layer in a neural network refers to the number of neurons or units in that layer. It represents the dimensionality of the hidden space or feature space that the layer can capture during the network's forward pass. In other words, the hidden size determines the number of activation values produced by the layer. The hidden size is a hyperparameter that can be adjusted when designing a neural network. It influences the network's capacity to learn complex patterns and representations. Larger hidden sizes allow the network to capture more intricate relationships in the data but also increase the number of parameters in the model, potentially leading to overfitting if not carefully controlled.\n",
    "\n",
    "In summary, the hidden size of a layer determines the number of neurons in that layer, representing the dimensionality of the hidden space where the network learns to extract features and representations from the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What does the t method do in PyTorch?\n",
    "\n",
    "In PyTorch, the `t()` method is used to transpose a tensor. Transposing a tensor means swapping its dimensions. For a 2D tensor (matrix), it effectively swaps the rows and columns. For tensors with more than two dimensions, it performs a general transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "Transposed Matrix:\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 2x3 matrix\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "\n",
    "# Transpose the matrix\n",
    "transposed_matrix = matrix.t()\n",
    "\n",
    "print(\"Original Matrix:\")\n",
    "print(matrix)\n",
    "\n",
    "print(\"\\nTransposed Matrix:\")\n",
    "print(transposed_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Why is matrix multiplication written in plain Python very slow?\n",
    "\n",
    "\n",
    "Matrix multiplication written in plain Python (using nested loops and list comprehensions) tends to be slow for several reasons:\n",
    "\n",
    "1. **Element-wise Operations:** In plain Python, operations are typically performed element-wise, which means looping through each element individually. This results in a high number of Python function calls, leading to significant overhead.\n",
    "   \n",
    "2. **Lack of Vectorization:** Python lists and loops are not optimized for numerical computations like matrix multiplication. NumPy, on the other hand, is a highly optimized library that leverages vectorized operations, allowing efficient computation on arrays.\n",
    "\n",
    "3. **Dynamic Typing Overhead:** Python is a dynamically typed language, and this dynamic typing introduces overhead. NumPy, being a statically typed library, is able to perform operations more efficiently because it knows the data types in advance.\n",
    "\n",
    "4. **Global Interpreter Lock (GIL):** Python's Global Interpreter Lock (GIL) can limit the parallel execution of threads. While NumPy operations release the GIL during computations, plain Python operations might not benefit from parallelism as effectively.\n",
    "\n",
    "5. **Memory Management:** NumPy uses efficient memory layouts for arrays, and its operations are optimized for cache usage. In contrast, in plain Python, memory access patterns may be less optimal, leading to slower performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. In matmul, why is ac==br?\n",
    "\n",
    "\n",
    "In the context of matrix multiplication, the condition `ac == br` ensures that the matrices involved in the multiplication operation are conformable, meaning the number of columns in the first matrix (A) must be equal to the number of rows in the second matrix (B). The standard matrix multiplication operation involves multiplying each element of a row in the first matrix by the corresponding element of a column in the second matrix and summing up these products. To accomplish this, the matrices must have compatible dimensions. For two matrices A and B being multiplied (where A has dimensions a x c and B has dimensions b x d), the resulting matrix C (the product of A and B) will have dimensions a x d.\n",
    "\n",
    "Therefore, to perform the multiplication, it's required that the number of columns in A (c) is equal to the number of rows in B (b), i.e., `ac == br`. If this condition is not satisfied, the matrices are not conformable for multiplication, and attempting to perform the operation will result in a shape mismatch error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "\n",
    "\n",
    "In Jupyter Notebook, you can measure the time taken for a single cell to execute using the `%time` or `%timeit` magic commands. These commands provide a convenient way to profile the execution time of a specific code cell.\n",
    "\n",
    "1. **`%time` Magic Command:** `%time` is used to measure the execution time of a single run of the cell's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "499999500000\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "result = 0\n",
    "for i in range(1000000):\n",
    "    result += i\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **`%timeit` Magic Command:** `%timeit` is used for more accurate and reliable timing by running the code multiple times and averaging the results. It is particularly useful for measuring short execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499999500000\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "result = 0\n",
    "for i in range(1000000):\n",
    "    result += i\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. What is elementwise arithmetic?\n",
    "\n",
    "\n",
    "Elementwise arithmetic refers to performing arithmetic operations on corresponding elements of two or more arrays or tensors. In the context of numerical computing, elementwise operations are applied independently to each element at corresponding positions in the input arrays, resulting in a new array with the same shape. The most common elementwise arithmetic operations include addition, subtraction, multiplication, division, exponentiation, and more. The key characteristic is that the operation is applied element by element, and the corresponding elements in the input arrays contribute to the corresponding elements in the output array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Write the PyTorch code to test whether every element of a is greater than the corresponding element of b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every element of 'a' is greater than the corresponding element of 'b': False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors a and b\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 2, 2])\n",
    "\n",
    "# Check if every element of a is greater than the corresponding element of b\n",
    "result = torch.all(a > b)\n",
    "\n",
    "# Print the result\n",
    "print(\"Every element of 'a' is greater than the corresponding element of 'b':\", result.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "\n",
    "\n",
    "A rank-0 tensor is essentially a scalar, representing a single value in a tensor. In PyTorch, a rank-0 tensor has zero dimensions, and it can be considered as a 0-dimensional array. While it might be colloquially referred to as a \"tensor,\" it essentially corresponds to a plain scalar value. To convert a rank-0 tensor to a plain Python data type, you can use the `item()` method. This method extracts the scalar value from the tensor and returns it as a native Python data type.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank-0 Tensor: tensor(42)\n",
      "Converted to Python Scalar: 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a rank-0 tensor (scalar)\n",
    "rank_0_tensor = torch.tensor(42)\n",
    "\n",
    "# Convert the rank-0 tensor to a plain Python data type\n",
    "python_scalar = rank_0_tensor.item()\n",
    "\n",
    "# Print the result\n",
    "print(\"Rank-0 Tensor:\", rank_0_tensor)\n",
    "print(\"Converted to Python Scalar:\", python_scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. How does elementwise arithmetic help us speed up matmul?\n",
    "\n",
    "\n",
    "Elementwise arithmetic does not directly speed up matrix multiplication (`matmul`), but it is an integral part of the overall optimization strategy for improving the performance of matrix multiplication in numerical computing libraries. The process involves taking advantage of parallelism, vectorization, and optimized low-level implementations.\n",
    "\n",
    "Here's how elementwise arithmetic contributes to the optimization of `matmul`:\n",
    "\n",
    "1. **Parallelism and Vectorization:**\n",
    "   - Modern CPUs and GPUs are capable of performing parallel operations on arrays of data. Elementwise arithmetic operations can be parallelized, allowing multiple elements to be processed simultaneously.\n",
    "   - When performing matrix multiplication, the individual multiplications and additions involved can be parallelized across multiple processor cores or SIMD (Single Instruction, Multiple Data) units, leading to faster computation.\n",
    "\n",
    "2. **Vectorized Implementations:**\n",
    "   - Libraries like NumPy, PyTorch, and TensorFlow are designed to take advantage of vectorized implementations. These libraries are written in low-level languages (C or Fortran), and they leverage efficient implementations of elementwise operations for various hardware architectures.\n",
    "   - Matrix multiplication involves a combination of elementwise multiplications and additions. By using optimized and vectorized implementations of these operations, the overall performance of matrix multiplication is improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. What are the broadcasting rules?\n",
    "\n",
    "Broadcasting is a mechanism in array-oriented computing libraries (such as NumPy, PyTorch, and TensorFlow) that allows operations on arrays of different shapes and sizes to be performed without explicitly copying the data. Broadcasting follows a set of rules to determine how the smaller array can be \"broadcast\" across the larger array to make their shapes compatible for elementwise operations. The broadcasting rules are as follows:\n",
    "\n",
    "1. **Rule 1: Dimensions of the Arrays:**\n",
    "   - If the arrays have a different number of dimensions, pad the smaller array's shape on its left side with ones until the shapes have the same length.\n",
    "\n",
    "2. **Rule 2: Size of Dimensions:**\n",
    "   - If, after applying Rule 1, the sizes of the dimensions are still different, the arrays are not broadcast-compatible.\n",
    "\n",
    "3. **Rule 3: Size 1 Dimensions:**\n",
    "   - If, after applying Rule 1, the sizes of the dimensions are the same or one of them is 1, the arrays are broadcast-compatible along that dimension.\n",
    "\n",
    "   Example:\n",
    "   ```python\n",
    "   A:     4 x 3\n",
    "   B:         3   # Rule 1: Pad with ones on the left\n",
    "   Result: 4 x 3  # Rule 3: Compatible dimensions\n",
    "   ```\n",
    "\n",
    "4. **Rule 4: Size 1 Expansion:**\n",
    "   - If, after applying Rule 1, the sizes of the dimensions are different, but one of them is 1, the array with size 1 is expanded along that dimension to match the size of the other array.\n",
    "\n",
    "   Example:\n",
    "   ```python\n",
    "   A:     4 x 3\n",
    "   B:     1 x 3  # Rule 1: Pad with ones on the left\n",
    "   Result: 4 x 3  # Rule 4: Size 1 expansion along the second dimension of B\n",
    "   ```\n",
    "\n",
    "5. **Rule 5: Broadcasting along Multiple Dimensions:**\n",
    "   - If broadcasting is required along multiple dimensions, the sizes in all relevant dimensions must either be the same or one of them must be 1.\n",
    "\n",
    "   Example:\n",
    "   ```python\n",
    "   A:     4 x 3\n",
    "   B:     1 x 3  # Rule 1: Pad with ones on the left\n",
    "   Result: 4 x 3  # Rule 4: Size 1 expansion along the second dimension of B\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. What is expand_as? Show an example of how it can be used to match the results of broadcasting.\n",
    "\n",
    "\n",
    "In PyTorch, the `expand_as` method is used to expand the size of one tensor to match the size of another tensor by broadcasting. It is particularly useful when you want to perform elementwise operations on tensors with different shapes but want to align them according to broadcasting rules.\n",
    "\n",
    "Here's an example of how `expand_as` can be used to match the results of broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [2, 1].  Tensor sizes: [1, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m result_broadcast \u001b[38;5;241m=\u001b[39m tensor_a \u001b[38;5;241m+\u001b[39m tensor_b\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Broadcasting with expand_as\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m expanded_a \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m result_expand_as \u001b[38;5;241m=\u001b[39m expanded_a \u001b[38;5;241m+\u001b[39m tensor_b\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [2, 1].  Tensor sizes: [1, 3]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "tensor_a = torch.tensor([[1, 2, 3]])\n",
    "tensor_b = torch.tensor([[4], [5]])\n",
    "\n",
    "# Broadcasting without expand_as\n",
    "result_broadcast = tensor_a + tensor_b\n",
    "\n",
    "# Broadcasting with expand_as\n",
    "expanded_a = tensor_a.expand_as(tensor_b)\n",
    "result_expand_as = expanded_a + tensor_b\n",
    "\n",
    "# Print the results\n",
    "print(\"Result without expand_as (broadcasting):\")\n",
    "print(result_broadcast)\n",
    "\n",
    "print(\"\\nResult with expand_as:\")\n",
    "print(result_expand_as)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
