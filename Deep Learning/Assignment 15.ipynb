{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deep Learning.\n",
    "### a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the number of neurons in each hidden layer\n",
    "hidden_units = 100\n",
    "\n",
    "# Define the number of output classes (if applicable)\n",
    "num_classes = 10  # Change according to your task\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (input_size,)  # input_size is the size of your input features\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=input_shape),\n",
    "    tf.keras.layers.Dense(hidden_units, kernel_initializer='he_normal', activation='elu'),\n",
    "    tf.keras.layers.Dense(hidden_units, kernel_initializer='he_normal', activation='elu'),\n",
    "    tf.keras.layers.Dense(hidden_units, kernel_initializer='he_normal', activation='elu'),\n",
    "    tf.keras.layers.Dense(hidden_units, kernel_initializer='he_normal', activation='elu'),\n",
    "    tf.keras.layers.Dense(hidden_units, kernel_initializer='he_normal', activation='elu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Change loss function according to your task\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rushabh.Patel2\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\Rushabh.Patel2\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Rushabh.Patel2\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "765/765 [==============================] - 7s 6ms/step - loss: 0.1175 - accuracy: 0.9620 - val_loss: 0.0595 - val_accuracy: 0.9796\n",
      "Epoch 2/50\n",
      "  8/765 [..............................] - ETA: 5s - loss: 0.0609 - accuracy: 0.9844 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rushabh.Patel2\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/765 [==============================] - 4s 6ms/step - loss: 0.0608 - accuracy: 0.9815 - val_loss: 0.0400 - val_accuracy: 0.9861\n",
      "Epoch 3/50\n",
      "765/765 [==============================] - 5s 6ms/step - loss: 0.0412 - accuracy: 0.9862 - val_loss: 0.0521 - val_accuracy: 0.9859\n",
      "Epoch 4/50\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.0363 - accuracy: 0.9888 - val_loss: 0.0517 - val_accuracy: 0.9861\n",
      "Epoch 5/50\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.0269 - accuracy: 0.9917 - val_loss: 0.0521 - val_accuracy: 0.9873\n",
      "Epoch 6/50\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.0236 - accuracy: 0.9933 - val_loss: 0.0530 - val_accuracy: 0.9887\n",
      "Epoch 7/50\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.0201 - accuracy: 0.9935 - val_loss: 0.0535 - val_accuracy: 0.9873\n",
      "Epoch 8/50\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.0200 - accuracy: 0.9934 - val_loss: 0.0425 - val_accuracy: 0.9900\n",
      "Epoch 9/50\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.0167 - accuracy: 0.9953 - val_loss: 0.0619 - val_accuracy: 0.9873\n",
      "Epoch 10/50\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.0147 - accuracy: 0.9956 - val_loss: 0.0415 - val_accuracy: 0.9912\n",
      "Epoch 11/50\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 0.0171 - accuracy: 0.9952 - val_loss: 0.0521 - val_accuracy: 0.9900\n",
      "Epoch 12/50\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.0116 - accuracy: 0.9964 - val_loss: 0.0724 - val_accuracy: 0.9873\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Filter digits 0 to 4\n",
    "train_mask = (train_labels < 5)\n",
    "test_mask = (test_labels < 5)\n",
    "train_images, train_labels = train_images[train_mask], train_labels[train_mask]\n",
    "test_images, test_labels = test_images[test_mask], test_labels[test_mask]\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Build the DNN\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    Dense(5, activation='softmax')  # Softmax output layer with five neurons\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up callbacks for checkpointing and early stopping\n",
    "checkpoint_cb = ModelCheckpoint(\"model_checkpoint.h5\", save_best_only=True)\n",
    "early_stopping_cb = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=50, validation_split=0.2,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"final_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Tune the hyperparameters using cross-validation and see what precision you can achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ModelCheckpoint\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load MNIST dataset\u001b[39;00m\n\u001b[0;32m     10\u001b[0m (X_train_full, y_train_full), (X_test, y_test) \u001b[38;5;241m=\u001b[39m mnist\u001b[38;5;241m.\u001b[39mload_data()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Filter for digits 0 to 4\n",
    "mask_train = (y_train_full <= 4)\n",
    "mask_test = (y_test <= 4)\n",
    "\n",
    "X_train, y_train = X_train_full[mask_train], y_train_full[mask_train]\n",
    "X_test, y_test = X_test[mask_test], y_test[mask_test]\n",
    "\n",
    "# Flatten and normalize the pixel values\n",
    "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "# One-hot encode the target labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=5)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=5)\n",
    "\n",
    "# Define the DNN model function\n",
    "def create_dnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='elu', kernel_initializer='he_normal', input_shape=(784,)))\n",
    "    model.add(Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and compile the DNN model\n",
    "dnn_model = create_dnn_model()\n",
    "\n",
    "# Set up callbacks for early stopping and model checkpoint\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint_cb = ModelCheckpoint(\"dnn_model.h5\", save_best_only=True)\n",
    "\n",
    "# Train the model on the training set\n",
    "history = dnn_model.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping_cb, model_checkpoint_cb])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = dnn_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Hyperparameter tuning using cross-validation\n",
    "param_grid = {'batch_size': [32, 64, 128], 'epochs': [50, 100, 150]}\n",
    "model = KerasClassifier(build_fn=create_dnn_model, verbose=0)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='precision_weighted', verbose=2)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and corresponding precision score\n",
    "print(\"Best Parameters: \", grid_result.best_params_)\n",
    "print(\"Best Precision Score: \", grid_result.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
